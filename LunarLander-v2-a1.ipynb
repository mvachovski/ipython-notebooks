{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import gym\n",
    "import gym.spaces\n",
    "import datetime as dt\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "env = gym.make(\"LunarLander-v2\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Agent:\n",
    "    \n",
    "    def __init__(self, name, input_layer, hsize, action_space_n, random_init=True):\n",
    "        self.name = name\n",
    "        self.hsize = hsize\n",
    "        self.action_space_n = action_space_n\n",
    "        with tf.variable_scope(name,reuse=False):\n",
    "            self.input = input_layer\n",
    "            self.h1 = tf.layers.dense(input_layer,hsize[0],activation=tf.nn.leaky_relu)\n",
    "            self.h2 = tf.layers.dense(self.h1,hsize[1],activation=tf.nn.leaky_relu)\n",
    "            self.out = tf.layers.dense(self.h2, action_space_n, activation=tf.nn.softmax)\n",
    "            \n",
    "    def zero(self):\n",
    "        tr_vars = self.train_vars()\n",
    "        for var in tr_vars:\n",
    "            sess.run(tf.assign(var, tf.zeros_like(var)))\n",
    "        \n",
    "    def from_population(self, agents, weights):\n",
    "        self.zero()\n",
    "        weight = 1. / len(agents)\n",
    "        train_vars = self.train_vars()\n",
    "        for i in range(len(agents)):\n",
    "        # for i in range(1):\n",
    "            current_agent_vars = agents[i].train_vars()\n",
    "            for j in range(len(current_agent_vars)):\n",
    "            # for j in range(1):\n",
    "                # print(\"train_vars[j]:\\n\", sess.run(train_vars[j]))\n",
    "                # print(\"weights[i]:\\n\", weights[i])\n",
    "                # print(\"current_agent_vars[j]:\\n\", sess.run(current_agent_vars[j]))\n",
    "                # print(\"train_vars[j] + weights[i] * current_agent_vars[j]:\\n\", sess.run(train_vars[j] + weights[i] * current_agent_vars[j]))\n",
    "                sess.run(tf.assign(train_vars[j], train_vars[j] + weights[i] * current_agent_vars[j]))\n",
    "                # print(\"train_vars[j]:\\n\", sess.run(train_vars[j]))\n",
    "        # print(\"from_population: \", sess.run(self.train_vars()[0])[0][0:5])\n",
    "                \n",
    "    def from_agent(self, agent, noise_dist = None):\n",
    "        self_train_vars = self.train_vars()\n",
    "        agent_train_vars = agent.train_vars()\n",
    "        for i in range(len(self_train_vars)):\n",
    "            if noise_dist is None:\n",
    "                sess.run(tf.assign(self_train_vars[i], agent_train_vars[i]))\n",
    "            else:\n",
    "                sess.run(tf.assign(self_train_vars[i], agent_train_vars[i] + noise_dist.sample(agent_train_vars[i].shape)))                    \n",
    "                \n",
    "    def compute(self, observation):\n",
    "        obs_np = np.array(observation).reshape((1, 8))\n",
    "        return sess.run(self.out, feed_dict= {X : obs_np})\n",
    "    \n",
    "    def train_vars(self):\n",
    "        return tf.get_collection(tf.GraphKeys.TRAINABLE_VARIABLES, self.name)\n",
    "    \n",
    "    def update(self, agent):\n",
    "        pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_agents = 10\n",
    "num_games_per_agent = 20\n",
    "max_steps = 400\n",
    "num_generations = 3\n",
    "hsizes = [24, 24]\n",
    "alpha = 0.03\n",
    "sigma = 0.03"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = tf.placeholder(tf.float32,[None, env.observation_space.shape[0]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "normal_dist = dist = tf.distributions.Normal(loc=0., scale=sigma)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "smith = Agent(name=\"Agent_Smith\", input_layer=X, hsize=hsizes, action_space_n=env.action_space.n)\n",
    "print(smith.train_vars())\n",
    "agents = []\n",
    "for i in range(num_agents):\n",
    "    # agents.append(agent(\"agent/%d\" % (i,), input_layer=X, hsize=hsizes, reuse=False, action_space_n=env.action_space.n))\n",
    "    agents.append(Agent(\"agent/%d\" % (i,), input_layer=X, hsize=hsizes, action_space_n=env.action_space.n))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sess = tf.Session()\n",
    "tf.global_variables_initializer().run(session=sess)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#for i in range(num_agents):\n",
    "#    train_vars = tf.get_collection(tf.GraphKeys.TRAINABLE_VARIABLES, \"agent/%d\" % (i,))\n",
    "#    print(sess.run(train_vars))\n",
    "#    print(\"======================\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def play(env, agent, num_games, max_steps, render):\n",
    "    rewards = np.zeros([num_games])\n",
    "    actions = [_ for _ in range(env.action_space.n)]\n",
    "    for g in range(num_games):\n",
    "        obs = env.reset()\n",
    "        #print(obs)\n",
    "        reward_accum = 0\n",
    "        for i in range(max_steps):\n",
    "            # obs_np = np.array(obs).reshape((1, 8))\n",
    "            # action_probabilities = sess.run(agent, feed_dict= {X : obs_np})\n",
    "            action_probabilities = agent.compute(obs)[0]\n",
    "            \n",
    "            #obs, r, done, info = env.step(env.action_space.sample())\n",
    "            action = np.random.choice(a=actions, p=action_probabilities)\n",
    "            obs, r, done, info = env.step(action)\n",
    "            if render == True:\n",
    "                env.render()\n",
    "            reward_accum += r\n",
    "            if done == True:\n",
    "                break\n",
    "        rewards[g] = reward_accum\n",
    "    return rewards.sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def softmax(x):\n",
    "    \"\"\"Compute softmax values for each sets of scores in x.\"\"\"\n",
    "    e_x = np.exp(x - np.max(x))\n",
    "    return e_x / e_x.sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "max_rewards = []\n",
    "avg_rewards = []\n",
    "start = dt.datetime.now()\n",
    "print(\"training started: \" + str(start))\n",
    "generation = 1\n",
    "# for generation in range(num_generations):\n",
    "stop_requested = False\n",
    "while(stop_requested == False):\n",
    "    rewards_np = np.zeros([num_agents])\n",
    "    for i in range(num_agents):\n",
    "        rewards_np[i] = play(env=env, agent=agents[i], num_games=num_games_per_agent, max_steps=max_steps, render=False)\n",
    "    print(generation, rewards_np.max(), rewards_np.mean())\n",
    "    max_rewards.append(rewards_np.max())\n",
    "    avg_rewards.append(rewards_np.mean())\n",
    "    ind = np.argmax(rewards_np)\n",
    "    weights = np.zeros((num_agents, ))\n",
    "    weights[ind] = 1.\n",
    "    smith.from_population(agents=agents, weights = weights  )\n",
    "    for i in range(num_agents):\n",
    "        # if i == 0:\n",
    "        #    print(sess.run(agents[i].train_vars()[0])[0][0:5])\n",
    "        agents[i].from_agent(agent=smith, noise_dist=normal_dist)\n",
    "        \n",
    "    with open(\"/home/martin/1.ctl\") as f:\n",
    "        cmd = f.readline()\n",
    "        if cmd.find(\"RUN\") < 0:\n",
    "            stop_requested = True\n",
    "    generation += 1\n",
    "\n",
    "end = dt.datetime.now()\n",
    "print(\"training ended: \" + str(end))\n",
    "print(\"total training time: \" + str(end - start))\n",
    "print(\"average generation training time: \", str((end - start).total_seconds() / len(max_rewards)) )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(range(len(avg_rewards)), avg_rewards)\n",
    "plt.plot(range(len(max_rewards)), max_rewards)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "obs = env.reset()\n",
    "actions = [_ for _ in range(env.action_space.n)]\n",
    "\n",
    "for i in range(500):\n",
    "    action_probabilities = smith.compute(obs)[0]\n",
    "            \n",
    "    # obs, r, done, info = env.step(env.action_space.sample())\n",
    "    # action = np.random.choice(a=actions, p=action_probabilities)\n",
    "    # o, r, done, info = env.step(env.action_space.sample())\n",
    "    \n",
    "    action = np.random.choice(a=actions, p=action_probabilities)\n",
    "    obs, r, done, info = env.step(action)\n",
    "    env.render()\n",
    "    # print(obs, r, done, info)\n",
    "    if done:\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
