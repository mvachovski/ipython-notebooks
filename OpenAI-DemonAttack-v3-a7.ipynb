{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import gym\n",
    "from collections import deque\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import tensorflow as tf\n",
    "from scipy import linalg\n",
    "from copy import deepcopy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def int2onehot(n, max_n):\n",
    "    res = np.zeros(max_n)\n",
    "    res[n] = 1.\n",
    "    return res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "num_frames = 2\n",
    "max_steps = 2000\n",
    "num_games = 100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def dim_reduce(a, V, keep_dim):\n",
    "    res = np.zeros([keep_dim])\n",
    "    for i in xrange(keep_dim):\n",
    "        res[i] = np.matmul(a, V[i])\n",
    "    return res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def red_train_agent2(training_epochs,red_games, commulative_rewards_np):\n",
    "    merged_summary = tf.summary.merge_all()\n",
    "    mx = commulative_rewards_np.max()\n",
    "    mean = commulative_rewards_np.mean()\n",
    "    amplitude = commulative_rewards_np.max() - commulative_rewards_np.min()\n",
    "    step = 1\n",
    "    for red_game, reward in zip(red_games, commulative_rewards_np):\n",
    "        step += 1\n",
    "        actions = np.array([_[1] for _ in red_game])\n",
    "        observations = np.array([_[0] for _ in red_game])\n",
    "        l = len(observations)\n",
    "        for n in xrange(training_epochs):\n",
    "            grads_buff = sess.run(red_grads, feed_dict={red_x:observations.reshape([l, red_Din]).astype(dtype=np.float32),\n",
    "                                                   red_y_:actions.reshape([l, Dout]).astype(dtype=np.float32)})\n",
    "            c = (reward - mean) / mean\n",
    "            sess.run(red_apply_grads, \\\n",
    "                feed_dict={red_grad1_ph:(c * grads_buff[0][0]), \\\n",
    "                                red_grad2_ph:(c * grads_buff[1][0]), \\\n",
    "                               red_grad3_ph:(c * grads_buff[2][0]), \\\n",
    "                               red_grad4_ph:(c * grads_buff[3][0])})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def play_random(num_games, max_steps, render):\n",
    "    games = deque()\n",
    "    env = gym.make(\"DemonAttack-ram-v3\")\n",
    "    commulative_rewards = deque()\n",
    "    for i in xrange(num_games):\n",
    "        #comp_observation = deque(maxlen = 3)\n",
    "        observation = np.array(env.reset())\n",
    "        comp_observation = deepcopy(observation) \n",
    "        comp_observations = deque()\n",
    "        actions = deque()\n",
    "        current_game = deque()\n",
    "        commulative_reward = 0\n",
    "        for j in xrange(max_steps):\n",
    "            if render:\n",
    "                env.render()            \n",
    "            old_observation = observation\n",
    "            action = env.action_space.sample()\n",
    "            observation, reward, done, info = env.step(action)\n",
    "            if len(comp_observation) == num_frames * 128:\n",
    "                comp_observations.append(deepcopy(comp_observation))\n",
    "            comp_observation = np.hstack((comp_observation, deepcopy(old_observation)))[-num_frames * 128:]\n",
    "            commulative_reward += reward\n",
    "            if(len(comp_observation) == num_frames * 128):\n",
    "                current_game.append((comp_observation, int2onehot(action, env.action_space.n), reward))\n",
    "                actions.append(int2onehot(action, env.action_space.n))\n",
    "                comp_observations.append(comp_observation)\n",
    "            if done:\n",
    "                break;\n",
    "        games.append(current_game)\n",
    "        commulative_rewards.append(commulative_reward)\n",
    "    #env.close()\n",
    "    #del env\n",
    "    return games, commulative_rewards"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def red_play(num_games, max_steps, render, V):\n",
    "    games = deque()\n",
    "    actions = [ _ for _ in xrange(0, 6)]\n",
    "    env = gym.make(\"DemonAttack-ram-v3\")\n",
    "    commulative_rewards = deque()\n",
    "    for i in xrange(num_games):\n",
    "        observation = env.reset()\n",
    "        comp_observation = deepcopy(observation)\n",
    "        current_game = deque()\n",
    "        commulative_reward = 0\n",
    "        for j in xrange(max_steps):\n",
    "            if render:\n",
    "                env.render()\n",
    "            if len(comp_observation) == num_frames * 128:\n",
    "                #print \"red_play: random_choice(probabilities)\", len(comp_observation)\n",
    "                red_observation = dim_reduce(comp_observation, V, red_Din)\n",
    "                probabilities = sess.run(red_y, \\\n",
    "                                     feed_dict={red_x:red_observation.reshape([1, \\\n",
    "                                        len(red_observation)]).astype(dtype=np.float32)})\n",
    "                action = np.random.choice(a=actions, p=probabilities[0])\n",
    "                observation, reward, done, info = env.step(action)\n",
    "                current_game.append((red_observation, int2onehot(action, env.action_space.n), reward))\n",
    "            else:\n",
    "                #print \"red_play: action_space.sample:\", len(comp_observation)\n",
    "                action = env.action_space.sample()\n",
    "                observation, reward, done, info = env.step(action)\n",
    "            commulative_reward += reward\n",
    "            \n",
    "            comp_observation = np.hstack((comp_observation, observation))[-num_frames * 128:]\n",
    "            if done:\n",
    "                break;\n",
    "        games.append(current_game)\n",
    "        commulative_rewards.append(commulative_reward)\n",
    "    #env.close()\n",
    "    #del env\n",
    "    return games, commulative_rewards"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "games, commulative_rewards = play_random(num_games=num_games, max_steps=2000, render=False)\n",
    "commulative_rewards_np = np.array(commulative_rewards)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "rewards_np = np.array( [_ for _ in commulative_rewards], dtype=np.float )\n",
    "print rewards_np.mean()\n",
    "h = np.histogram(rewards_np)\n",
    "plt.plot(h[1][:len(h[1]) - 1] + 10.5, h[0])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "game = games[commulative_rewards_np.argmax()]\n",
    "commulative_reward = commulative_rewards[commulative_rewards_np.argmax()]\n",
    "print game[0], commulative_reward\n",
    "print game[0][0], game[0][1], game[0][2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "observations = np.array([_[0] for _ in game], dtype=np.float)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "U, s, V = linalg.svd(observations[1:], full_matrices=True, compute_uv=True, overwrite_a=False, check_finite=True, lapack_driver='gesdd')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "print s"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "plt.plot(np.log10(s))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "lr = 1e-5\n",
    "training_epochs = 1\n",
    "D_hid1 = 150\n",
    "D_hid2 = 60\n",
    "red_Din = 160\n",
    "Din = 128\n",
    "Dout = 6"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "sess = tf.Session()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "red_x = tf.placeholder(tf.float32, [None, red_Din], name=\"red_x\")\n",
    "\n",
    "red_grad1_ph = tf.placeholder(tf.float32, name=\"red_grad1_ph\")\n",
    "red_grad2_ph = tf.placeholder(tf.float32, name=\"red_grad2_ph\")\n",
    "red_grad3_ph = tf.placeholder(tf.float32, name=\"red_grad3_ph\")\n",
    "red_grad4_ph = tf.placeholder(tf.float32, name=\"red_grad4_ph\")\n",
    "\n",
    "\n",
    "red_W1 = tf.Variable(tf.random_normal([red_Din, D_hid1], stddev=0.0001), name=\"red_W1\")\n",
    "red_b1 = tf.Variable(tf.random_normal([D_hid1], stddev=0.0001), name=\"red_b1\") \n",
    "\n",
    "red_W = tf.Variable(tf.random_normal([D_hid1, Dout], stddev=0.0001), name=\"red_W\")\n",
    "red_b = tf.Variable(tf.random_normal([Dout], stddev=0.0001), name=\"red_b\")\n",
    "\n",
    "red_H = tf.nn.relu(tf.matmul(red_x, red_W1) + red_b1)\n",
    "red_y = tf.nn.softmax(tf.matmul(red_H, red_W) + red_b)\n",
    "#red_y = tf.nn.softmax(tf.matmul(red_x, red_W) + red_b, name = \"red_y\")\n",
    "red_y_ = tf.placeholder(tf.float32, [None, Dout], name=\"red_y_\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#red_cross_entropy = tf.reduce_mean(-tf.reduce_sum(red_y_ * tf.log(red_y), reduction_indices=[1]))\n",
    "red_square_loss = tf.reduce_mean(tf.pow(red_y_ - red_y, 2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "red_adam = tf.train.AdamOptimizer(learning_rate=lr)\n",
    "\n",
    "#tvars = tf.trainable_variables()\n",
    "red_tvars = [red_W, red_b, red_W1, red_b1]\n",
    "\n",
    "red_grads = red_adam.compute_gradients(loss=red_square_loss, var_list=red_tvars)\n",
    "red_apply_grads = red_adam.apply_gradients(zip([red_grad1_ph, red_grad2_ph, red_grad3_ph, red_grad4_ph],red_tvars))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "sess.run(tf.global_variables_initializer())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "rewards = np.ndarray([0], dtype=np.float32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "num_iterations = 20\n",
    "for i in xrange(num_iterations):\n",
    "    print \"Iteration:\", i + 1, '/', num_iterations\n",
    "    red_games, commulative_rewards = red_play(num_games=num_games, max_steps=3000, render=False, V=V)\n",
    "    commulative_rewards_np = np.array(commulative_rewards)\n",
    "    rewards = np.hstack((rewards, commulative_rewards_np))\n",
    "    x = np.linspace(1, len(rewards), len(rewards))\n",
    "    plt.plot(x, rewards)\n",
    "    plt.show()\n",
    "    red_train_agent2(1, red_games, commulative_rewards_np)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "f = file(\"rewards-2.dat\", \"w\")\n",
    "np.save(f, rewards)\n",
    "f.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "f = file(\"V-matrix-2.dat\", \"w\")\n",
    "np.save(f, V)\n",
    "f.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "saver = tf.train.Saver()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "saver.save(sess, \"OpenAI-DemonAttack-v3-a7-2\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "f = file(\"checkpoints/V-matrix.dat\")\n",
    "V = np.load(f)\n",
    "f.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "print V"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "saver.restore(sess=sess, save_path=\"/home/martin/notebooks/checkpoints/OpenAI-DemonAttack-v3-a7\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
